{
    "Anthracnose": {
        "precision": 0.975,
        "recall": 1.0,
        "f1-score": 0.9873417721518988,
        "support": 39.0
    },
    "Bacterial_Canker": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 40.0
    },
    "Bacterial_Spot": {
        "precision": 0.9523809523809523,
        "recall": 1.0,
        "f1-score": 0.975609756097561,
        "support": 40.0
    },
    "Cutting_Weevil": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 40.0
    },
    "Die_Back": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 41.0
    },
    "Gall_Midge": {
        "precision": 1.0,
        "recall": 0.95,
        "f1-score": 0.9743589743589743,
        "support": 40.0
    },
    "Healthy": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 40.0
    },
    "Powdery_Mildew": {
        "precision": 1.0,
        "recall": 0.975,
        "f1-score": 0.9873417721518988,
        "support": 40.0
    },
    "Sooty_Mould": {
        "precision": 1.0,
        "recall": 1.0,
        "f1-score": 1.0,
        "support": 40.0
    },
    "accuracy": 0.9916666666666667,
    "macro avg": {
        "precision": 0.9919312169312169,
        "recall": 0.9916666666666667,
        "f1-score": 0.991628030528926,
        "support": 360.0
    },
    "weighted avg": {
        "precision": 0.9920006613756615,
        "recall": 0.9916666666666667,
        "f1-score": 0.9916631922729484,
        "support": 360.0
    },
    "regression_metrics": {
        "mse": 0.1638888888888889,
        "rmse": 0.40483192671637064,
        "r2_score": 0.9752512164656314,
        "accuracy": 0.9916666666666667
    }
}